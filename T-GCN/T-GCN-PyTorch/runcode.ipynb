{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.18.5)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (3.3.3)\n",
      "Collecting pytorch-lightning>=1.3.0\n",
      "  Downloading pytorch_lightning-1.5.10-py3-none-any.whl (527 kB)\n",
      "\u001b[K     |████████████████████████████████| 527 kB 3.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning>=1.3.0->-r requirements.txt (line 5)) (20.8)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning>=1.3.0->-r requirements.txt (line 5)) (3.7.4.3)\n",
      "Collecting pyDeprecate==0.3.1\n",
      "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
      "Collecting torch\n",
      "  Downloading torch-1.10.2-cp36-cp36m-manylinux1_x86_64.whl (881.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 881.9 MB 24 kB/s s eta 0:00:01     |██████                          | 163.3 MB 12.6 MB/s eta 0:00:57     |█████████████▊                  | 379.4 MB 7.9 MB/s eta 0:01:04\n",
      "\u001b[?25hCollecting torchmetrics>=0.3.0\n",
      "  Downloading torchmetrics-0.8.2-py3-none-any.whl (409 kB)\n",
      "\u001b[K     |████████████████████████████████| 409 kB 12.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
      "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 9.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting future>=0.17.1\n",
      "  Downloading future-0.18.3.tar.gz (840 kB)\n",
      "\u001b[K     |████████████████████████████████| 840 kB 12.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=17.0->pytorch-lightning>=1.3.0->-r requirements.txt (line 5)) (2.4.7)\n",
      "Collecting PyYAML>=5.1\n",
      "  Downloading PyYAML-6.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (603 kB)\n",
      "\u001b[K     |████████████████████████████████| 603 kB 13.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard>=2.2.0\n",
      "  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.9 MB 12.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.0->-r requirements.txt (line 5)) (0.11.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.0->-r requirements.txt (line 5)) (0.30.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.0->-r requirements.txt (line 5)) (3.14.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.0->-r requirements.txt (line 5)) (1.34.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.0->-r requirements.txt (line 5)) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.0->-r requirements.txt (line 5)) (3.3.3)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning>=1.3.0->-r requirements.txt (line 5)) (1.11.0)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.17.3-py2.py3-none-any.whl (178 kB)\n",
      "\u001b[K     |████████████████████████████████| 178 kB 13.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning>=1.3.0->-r requirements.txt (line 5)) (3.3.0)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 11.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[K     |████████████████████████████████| 83 kB 3.6 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting requests\n",
      "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 2.9 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.3.0->-r requirements.txt (line 5)) (2.6)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 12.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[K     |████████████████████████████████| 151 kB 607 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 11.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 11.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm>=4.41.0\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 5.0 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[K     |████████████████████████████████| 140 kB 10.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 2)) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 2)) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 2)) (8.1.0)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (9.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.5 MB 4.6 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting pytz>=2017.2\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[K     |████████████████████████████████| 502 kB 10.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting python-dotenv\n",
      "  Downloading python_dotenv-0.20.0-py3-none-any.whl (17 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.24.2-cp36-cp36m-manylinux2010_x86_64.whl (22.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 22.2 MB 7.0 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting scipy\n",
      "  Downloading scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.9 MB 10.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib>=0.11\n",
      "  Downloading joblib-1.1.1-py2.py3-none-any.whl (309 kB)\n",
      "\u001b[K     |████████████████████████████████| 309 kB 7.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.4-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (946 kB)\n",
      "\u001b[K     |████████████████████████████████| 946 kB 6.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.3.0->-r requirements.txt (line 5)) (20.3.0)\n",
      "Collecting asynctest==0.13.0\n",
      "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.2.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (191 kB)\n",
      "\u001b[K     |████████████████████████████████| 191 kB 8.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna-ssl>=1.0\n",
      "  Downloading idna-ssl-1.1.0.tar.gz (3.4 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-5.2.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (159 kB)\n",
      "\u001b[K     |████████████████████████████████| 159 kB 11.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (270 kB)\n",
      "\u001b[K     |████████████████████████████████| 270 kB 8.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dataclasses\n",
      "  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning>=1.3.0->-r requirements.txt (line 5)) (3.4.0)\n",
      "Collecting importlib-resources\n",
      "  Downloading importlib_resources-5.4.0-py3-none-any.whl (28 kB)\n",
      "Collecting setuptools==59.5.0\n",
      "  Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\n",
      "\u001b[K     |████████████████████████████████| 952 kB 8.7 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: future, idna-ssl\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=494240 sha256=a186334b41dd06457684403c37799f66f9df6d6cd839fcee2daa5b7d11f62c02\n",
      "  Stored in directory: /root/.cache/pip/wheels/63/f1/0c/e56d12b3804345ce5ba34279cbfe583ecafdd1401551457330\n",
      "  Building wheel for idna-ssl (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-py3-none-any.whl size=3949 sha256=15710d94b205f440f6a29e6035db67b2614e8cacbad4733c572c0a9aaf56d27f\n",
      "  Stored in directory: /root/.cache/pip/wheels/6a/f5/9c/f8331a854f7a8739cf0e74c13854e4dd7b1af11b04fe1dde13\n",
      "Successfully built future idna-ssl\n",
      "Installing collected packages: urllib3, pyasn1, charset-normalizer, certifi, rsa, requests, pyasn1-modules, oauthlib, multidict, frozenlist, cachetools, yarl, requests-oauthlib, idna-ssl, google-auth, dataclasses, asynctest, async-timeout, aiosignal, torch, tensorboard-plugin-wit, tensorboard-data-server, setuptools, pyDeprecate, importlib-resources, google-auth-oauthlib, fsspec, aiohttp, tqdm, torchmetrics, threadpoolctl, tensorboard, scipy, PyYAML, pytz, joblib, future, scikit-learn, pytorch-lightning, python-dotenv, pandas\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 51.1.1\n",
      "    Uninstalling setuptools-51.1.1:\n",
      "      Successfully uninstalled setuptools-51.1.1\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 1.15.0\n",
      "    Uninstalling tensorboard-1.15.0:\n",
      "      Successfully uninstalled tensorboard-1.15.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 1.15.5 requires tensorboard<1.16.0,>=1.15.0, but you have tensorboard 2.10.1 which is incompatible.\u001b[0m\n",
      "Successfully installed PyYAML-6.0 aiohttp-3.8.4 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 cachetools-4.2.4 certifi-2022.12.7 charset-normalizer-2.0.12 dataclasses-0.8 frozenlist-1.2.0 fsspec-2022.1.0 future-0.18.3 google-auth-2.17.3 google-auth-oauthlib-0.4.6 idna-ssl-1.1.0 importlib-resources-5.4.0 joblib-1.1.1 multidict-5.2.0 oauthlib-3.2.2 pandas-1.1.5 pyDeprecate-0.3.1 pyasn1-0.5.0 pyasn1-modules-0.3.0 python-dotenv-0.20.0 pytorch-lightning-1.5.10 pytz-2023.3 requests-2.27.1 requests-oauthlib-1.3.1 rsa-4.9 scikit-learn-0.24.2 scipy-1.5.4 setuptools-59.5.0 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 threadpoolctl-3.1.0 torch-1.10.2 torchmetrics-0.8.2 tqdm-4.64.1 urllib3-1.26.15 yarl-1.7.2\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run code with setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('GRU', 'LSTM', 'GCN', 'TGCN', 'TGCN_LSTM', 'TGCN_UGRNN')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"GRU\", \"LSTM\", \"GCN\", \"TGCN\",\"TGCN_LSTM\",\"TGCN_UGRNN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mse_with_regularizer', 'mse_with_regularizer_l1']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"mse_with_regularizer\",\"mse_with_regularizer_l1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"losloop\",\"PEMS08\",\"PEMS04\",\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: losloop  Loss: mse_with_regularizer_entropy  Model: TGCN Layer 2: False\n",
      "b'--------------------------------------------------------------------------------\\n'\n",
      "b'DATALOADER:0 VALIDATE RESULTS\\n'\n",
      "b\"{'ExplainedVar': 0.2797713279724121,\\n\"\n",
      "b\" 'MAE': 12.394514083862305,\\n\"\n",
      "b\" 'R2': 0.16201472282409668,\\n\"\n",
      "b\" 'RMSE': 16.684919357299805,\\n\"\n",
      "b\" 'accuracy': 0.7159579396247864,\\n\"\n",
      "b\" 'val_loss': 448240805740544.0}\\n\"\n",
      "b'--------------------------------------------------------------------------------\\n'\n",
      "Dataset: losloop  Loss: mse  Model: TGCN Layer 2: False\n"
     ]
    }
   ],
   "source": [
    "from subprocess import Popen,PIPE\n",
    "model = [\"TGCN\"]\n",
    "max_epochs = \"1\"\n",
    "learning_rate = \"0.001\" \n",
    "weight_decay =\"0\"\n",
    "enable_progress_bar = \"False\"\n",
    "batch_size = \"64\"\n",
    "hidden_dim = \"64\"\n",
    "cell_dim = \"64\"\n",
    "dropout = \"0\"\n",
    "layer_2 = \"1\"\n",
    "loss = [\"mse\"]\n",
    "data = [\"losloop\"]\n",
    "gpus = \"0\"\n",
    "for i in data:\n",
    "    for j in loss:\n",
    "        for k in model:\n",
    "            for l in layer_2:\n",
    "                print(\"Dataset:\",i,\" Loss:\",j,\" Model:\",k,\"Layer 2:\",l)\n",
    "                cmd_str = [\"python\",\n",
    "                       \"main.py\",\n",
    "                       \"--enable_progress_bar\",enable_progress_bar,\n",
    "                       \"--model_name\",k,\n",
    "                       \"--max_epochs\",max_epochs,\n",
    "                       \"--learning_rate\",learning_rate,\n",
    "                       \"--weight_decay\",weight_decay,\n",
    "                       \"--batch_size\",batch_size,\n",
    "                       \"--hidden_dim\",hidden_dim,\n",
    "                       \"--cell_dim\",cell_dim,\n",
    "                       \"--dropout\",dropout,\n",
    "                       \"--loss\",j,\n",
    "                       \"--layer_2\",l,\n",
    "                       \"--data\",i,\n",
    "                       \"--gpus\",gpus,\n",
    "                      ]\n",
    "                p = Popen(cmd_str,stdout=PIPE)\n",
    "                for m in p.stdout:\n",
    "                    print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test one with time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[2023-05-06 05:57:44,318 INFO]\u001b[0m{'logger': True, 'checkpoint_callback': None, 'enable_checkpointing': True, 'default_root_dir': None, 'gradient_clip_val': None, 'gradient_clip_algorithm': None, 'process_position': 0, 'num_nodes': 1, 'num_processes': 1, 'devices': None, 'gpus': None, 'auto_select_gpus': False, 'tpu_cores': None, 'ipus': None, 'log_gpu_memory': None, 'progress_bar_refresh_rate': None, 'enable_progress_bar': True, 'overfit_batches': 0.0, 'track_grad_norm': -1, 'check_val_every_n_epoch': 1, 'fast_dev_run': False, 'accumulate_grad_batches': None, 'max_epochs': 2, 'min_epochs': None, 'max_steps': -1, 'min_steps': None, 'max_time': None, 'limit_train_batches': 1.0, 'limit_val_batches': 1.0, 'limit_test_batches': 1.0, 'limit_predict_batches': 1.0, 'val_check_interval': 1.0, 'flush_logs_every_n_steps': None, 'log_every_n_steps': 50, 'accelerator': None, 'strategy': None, 'sync_batchnorm': False, 'precision': 32, 'enable_model_summary': True, 'weights_summary': 'top', 'weights_save_path': None, 'num_sanity_val_steps': 2, 'resume_from_checkpoint': None, 'profiler': None, 'benchmark': False, 'deterministic': False, 'reload_dataloaders_every_n_epochs': 0, 'reload_dataloaders_every_epoch': False, 'auto_lr_find': False, 'replace_sampler_ddp': True, 'detect_anomaly': False, 'auto_scale_batch_size': False, 'prepare_data_per_node': None, 'plugins': None, 'amp_backend': 'native', 'amp_level': None, 'move_metrics_to_cpu': False, 'multiple_trainloader_mode': 'max_size_cycle', 'stochastic_weight_avg': False, 'terminate_on_nan': None, 'data': 'losloop', 'model_name': 'TGCN', 'settings': 'supervised', 'log_path': None, 'send_email': False, 'self_attention': 1, 'batch_size': 64, 'seq_len': 12, 'pre_len': 12, 'split_ratio': 0.8, 'normalize': True, 'svd': 0, 'hidden_dim': 64, 'dropout': 0, 'cell_dim': 64, 'learning_rate': 0.001, 'weight_decay': 0.0, 'loss': 'mse'}\n",
      "hej\n",
      "\u001b[31m[2023-05-06 05:57:44,537 INFO]\u001b[0mGPU available: False, used: False\n",
      "\u001b[31m[2023-05-06 05:57:44,537 INFO]\u001b[0mTPU available: False, using: 0 TPU cores\n",
      "\u001b[31m[2023-05-06 05:57:44,537 INFO]\u001b[0mIPU available: False, using: 0 IPUs\n",
      "\u001b[31m[2023-05-06 05:57:48,223 INFO]\u001b[0m\n",
      "  | Name      | Type   | Params\n",
      "-------------------------------------\n",
      "0 | model     | TGCN   | 54.0 K\n",
      "1 | regressor | Linear | 780   \n",
      "-------------------------------------\n",
      "54.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "54.8 K    Total params\n",
      "0.219     Total estimated model params size (MB)\n",
      "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "Epoch 0:  27%|███▏        | 7/26 [00:07<00:21,  1.12s/it, loss=0.768, v_num=158]^C\n",
      "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py:1399: UserWarning: `.validate(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `validate(ckpt_path='best')` to use and best model checkpoint and avoid this warning or `ckpt_path=trainer.checkpoint_callback.last_model_path` to use the last model.\n",
      "  f\"`.{fn}(ckpt_path=None)` was called without a model.\"\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 113, in <module>\n",
      "    results = main(args)\n",
      "  File \"main.py\", line 73, in main\n",
      "    results = globals()[\"main_\" + args.settings](args)\n",
      "  File \"main.py\", line 67, in main_supervised\n",
      "    results = trainer.validate(datamodule=dm)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 821, in validate\n",
      "    return self._call_and_handle_interrupt(self._validate_impl, model, dataloaders, ckpt_path, verbose, datamodule)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 685, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 860, in _validate_impl\n",
      "    ckpt_path, model_provided=model_provided, model_connected=self.lightning_module is not None\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1420, in __set_ckpt_path\n",
      "    f'`.{fn}(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model.'\n",
      "pytorch_lightning.utilities.exceptions.MisconfigurationException: `.validate(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model.\n",
      "CPU times: user 250 ms, sys: 76.5 ms, total: 326 ms\n",
      "Wall time: 17.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!python main.py \\\n",
    "--model_name TGCN \\\n",
    "--data losloop \\\n",
    "--max_epochs 2 \\\n",
    "--learning_rate 0.001 \\\n",
    "--weight_decay 0 \\\n",
    "--batch_size 64 \\\n",
    "--hidden_dim 64 \\\n",
    "--self_attention 1 \\\n",
    "--loss \"mse\" \\\n",
    "--seq_len 12 \\\n",
    "--pre_len 12 \\\n",
    "--svd 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results to save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATALOADER:0 VALIDATE RESULTS\n",
    "{'ExplainedVar': 0.6397898197174072,\n",
    " 'MAE': 5.414433479309082,\n",
    " 'R2': 0.6379294991493225,\n",
    " 'RMSE': 8.367987632751465,\n",
    " 'accuracy': 0.8574607968330383,\n",
    " 'val_loss': 70.0232162475586}\n",
    "--------------------------------------------------------------------------------\n",
    "Validating: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.25it/s]\n",
    "\n",
    "CPU times: user 14.8 s, sys: 3.32 s, total: 18.1 s\n",
    "Wall time: 17min 59s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "import time\n",
    "\n",
    "def svd_low_rank_approximation(matrix,k):\n",
    "    # Perform SVD on the matrix\n",
    "    u, s, v = torch.linalg.svd(matrix, full_matrices=False)\n",
    "    \n",
    "    k=0\n",
    "    # Truncate the singular values and matrices to rank k\n",
    "    s_k = s[:k]\n",
    "    u_k = u[:, :k]\n",
    "    v_k = v[:, :k]\n",
    "    return u_k.mm(torch.diag(s_k)),v_k.t()\n",
    "\n",
    "def adjmat(adj):\n",
    "    return torch.mm(torch.mm(adj[1], torch.diag(adj[0])), adj[2].t())\n",
    "\n",
    "PEMS08_adj = pd.read_csv(r'data/pems08_adj.csv',header=None)\n",
    "adj = np.array(PEMS08_adj,dtype=np.float32)\n",
    "adj = torch.tensor(adj)\n",
    "\n",
    "data = np.load(\"data/pems08.npz\")\n",
    "data = data.f.data\n",
    "df_data = pd.DataFrame(data[:,:,0])\n",
    "df_data = df_data.values.tolist()\n",
    "feat = np.array(df_data, dtype=np.float32)\n",
    "feat = torch.tensor(feat)\n",
    "\n",
    "for i in range(1):\n",
    "    adjsvd= svd_low_rank_approximation(feat,115)\n",
    "    adjmatrix1= adjsvd\n",
    "for i in range(1):\n",
    "    adjsvd= svd_low_rank_approximation(feat,170)\n",
    "    adjmatrix= adjsvd\n",
    "q=adjmatrix1[1].matmul(adj)\n",
    "l=adjmatrix1[0].matmul(q)\n",
    "start = time.time()\n",
    "\n",
    "print(\"svd s u v = \",adjmatrix1[0].matmul(adjmatrix1[1].matmul( adj))\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "start = time.time()\n",
    "print(\"svd s u v = \",l==adjmatrix1[0].matmul(adjmatrix1[1].matmul( adj)))\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "import pandas as pd\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def flip_signs(A, B):\n",
    "    \"\"\"\n",
    "    utility function for resolving the sign ambiguity in SVD\n",
    "    http://stats.stackexchange.com/q/34396/115202\n",
    "    \"\"\"\n",
    "    signs = np.sign(A) * np.sign(B)\n",
    "    return A, B * signs\n",
    "\n",
    "\n",
    "# Let the data matrix X be of n x p size,\n",
    "# where n is the number of samples and p is the number of variables\n",
    "n, p = 170, 170\n",
    "adj_df = pd.read_csv(\"data/pems08_adj.csv\", header=None)\n",
    "X = np.array(adj_df, dtype=np.float32)\n",
    "X -= np.mean(X,axis=0)\n",
    "\n",
    "# the p x p covariance matrix\n",
    "C = np.cov(X, rowvar=False)\n",
    "print(\"C = \\n\", C)\n",
    "# C is a symmetric matrix and so it can be diagonalized:\n",
    "l, principal_axes = la.eig(C)\n",
    "# sort results wrt. eigenvalues\n",
    "idx = l.argsort()[::-1]\n",
    "l, principal_axes = l[idx], principal_axes[:, idx]\n",
    "# the eigenvalues in decreasing order\n",
    "print(\"l = \\n\", l)\n",
    "# a matrix of eigenvectors (each column is an eigenvector)\n",
    "print(\"V = \\n\", principal_axes)\n",
    "# projections of X on the principal axes are called principal components\n",
    "principal_components = X.dot(principal_axes)\n",
    "print(\"Y = \\n\", principal_components)\n",
    "\n",
    "# we now perform singular value decomposition of X\n",
    "# \"economy size\" (or \"thin\") SVD\n",
    "U, s, Vt = la.svd(X, full_matrices=False)\n",
    "V = Vt.T\n",
    "S = np.diag(s)\n",
    "\n",
    "# 1) then columns of V are principal directions/axes.\n",
    "#assert np.allclose(V, principal_axes)\n",
    "\n",
    "# 2) columns of US are principal components\n",
    "assert np.allclose(*flip_signs(U.dot(S), principal_components))\n",
    "\n",
    "# 3) singular values are related to the eigenvalues of covariance matrix\n",
    "assert np.allclose((s ** 2) / (n - 1), l)\n",
    "\n",
    "# 8) dimensionality reduction\n",
    "k = 2\n",
    "PC_k = principal_components[:, 0:k]\n",
    "US_k = U[:, 0:k].dot(S[0:k, 0:k])\n",
    "assert np.allclose(*flip_signs(PC_k, US_k))\n",
    "\n",
    "# 10) we used \"economy size\" (or \"thin\") SVD\n",
    "assert U.shape == (n, p)\n",
    "assert S.shape == (p, p)\n",
    "assert V.shape == (p, p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
