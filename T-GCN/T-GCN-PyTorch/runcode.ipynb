{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run code with setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('GRU', 'LSTM', 'GCN', 'TGCN', 'TGCN_LSTM', 'TGCN_UGRNN')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"GRU\", \"LSTM\", \"GCN\", \"TGCN\",\"TGCN_LSTM\",\"TGCN_UGRNN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mse_with_regularizer', 'mse_with_regularizer_l1']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"mse_with_regularizer\",\"mse_with_regularizer_l1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"losloop\",\"PEMS08\",\"PEMS04\",\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: losloop  Loss: mse_with_regularizer_entropy  Model: TGCN Layer 2: False\n",
      "b'--------------------------------------------------------------------------------\\n'\n",
      "b'DATALOADER:0 VALIDATE RESULTS\\n'\n",
      "b\"{'ExplainedVar': 0.2797713279724121,\\n\"\n",
      "b\" 'MAE': 12.394514083862305,\\n\"\n",
      "b\" 'R2': 0.16201472282409668,\\n\"\n",
      "b\" 'RMSE': 16.684919357299805,\\n\"\n",
      "b\" 'accuracy': 0.7159579396247864,\\n\"\n",
      "b\" 'val_loss': 448240805740544.0}\\n\"\n",
      "b'--------------------------------------------------------------------------------\\n'\n",
      "Dataset: losloop  Loss: mse  Model: TGCN Layer 2: False\n"
     ]
    }
   ],
   "source": [
    "from subprocess import Popen,PIPE\n",
    "model = [\"TGCN\"]\n",
    "max_epochs = \"1\"\n",
    "learning_rate = \"0.001\" \n",
    "weight_decay =\"0\"\n",
    "enable_progress_bar = \"False\"\n",
    "batch_size = \"64\"\n",
    "hidden_dim = \"64\"\n",
    "cell_dim = \"64\"\n",
    "dropout = \"0\"\n",
    "layer_2 = [\"False\"]\n",
    "loss = [\"mse_with_regularizer_entropy\",\"mse\"]\n",
    "data = [\"losloop\"]\n",
    "gpus = \"0\"\n",
    "for i in data:\n",
    "    for j in loss:\n",
    "        for k in model:\n",
    "            for l in layer_2:\n",
    "                print(\"Dataset:\",i,\" Loss:\",j,\" Model:\",k,\"Layer 2:\",l)\n",
    "                cmd_str = [\"python\",\n",
    "                       \"main.py\",\n",
    "                       \"--enable_progress_bar\",enable_progress_bar,\n",
    "                       \"--model_name\",k,\n",
    "                       \"--max_epochs\",max_epochs,\n",
    "                       \"--learning_rate\",learning_rate,\n",
    "                       \"--weight_decay\",weight_decay,\n",
    "                       \"--batch_size\",batch_size,\n",
    "                       \"--hidden_dim\",hidden_dim,\n",
    "                       \"--cell_dim\",hidden_dim,\n",
    "                       \"--dropout\",dropout,\n",
    "                       \"--loss\",j,\n",
    "                       \"--layer_2\",layer_2[0],\n",
    "                       \"--data\",i,\n",
    "                       \"--gpus\",gpus,\n",
    "                      ]\n",
    "                p = Popen(cmd_str,stdout=PIPE)\n",
    "                for m in p.stdout:\n",
    "                    print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test one with time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "!python main.py \\\n",
    "--model_name TGCN \\\n",
    "--data losloop \\\n",
    "--max_epochs 5 \\\n",
    "--learning_rate 0.001 \\\n",
    "--weight_decay 0 \\\n",
    "--batch_size 32 \\\n",
    "--hidden_dim 64 \\\n",
    "--loss \"mse\" \\\n",
    "--pre_len 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results to save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATALOADER:0 VALIDATE RESULTS\n",
    "{'ExplainedVar': 0.4844670295715332,\n",
    " 'MAE': 6.819440841674805,\n",
    " 'R2': 0.4796629548072815,\n",
    " 'RMSE': 10.107505798339844,\n",
    " 'accuracy': 0.8277632594108582,\n",
    " 'val_loss': 102.16167449951172}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "import time\n",
    "\n",
    "def svd_low_rank_approximation(matrix,k):\n",
    "    # Perform SVD on the matrix\n",
    "    u, s, v = torch.linalg.svd(matrix, full_matrices=False)\n",
    "    \n",
    "    k=0\n",
    "    # Truncate the singular values and matrices to rank k\n",
    "    s_k = s[:k]\n",
    "    u_k = u[:, :k]\n",
    "    v_k = v[:, :k]\n",
    "    return u_k.mm(torch.diag(s_k)),v_k.t()\n",
    "\n",
    "def adjmat(adj):\n",
    "    return torch.mm(torch.mm(adj[1], torch.diag(adj[0])), adj[2].t())\n",
    "\n",
    "PEMS08_adj = pd.read_csv(r'data/pems08_adj.csv',header=None)\n",
    "adj = np.array(PEMS08_adj,dtype=np.float32)\n",
    "adj = torch.tensor(adj)\n",
    "\n",
    "data = np.load(\"data/pems08.npz\")\n",
    "data = data.f.data\n",
    "df_data = pd.DataFrame(data[:,:,0])\n",
    "df_data = df_data.values.tolist()\n",
    "feat = np.array(df_data, dtype=np.float32)\n",
    "feat = torch.tensor(feat)\n",
    "\n",
    "for i in range(1):\n",
    "    adjsvd= svd_low_rank_approximation(feat,115)\n",
    "    adjmatrix1= adjsvd\n",
    "for i in range(1):\n",
    "    adjsvd= svd_low_rank_approximation(feat,170)\n",
    "    adjmatrix= adjsvd\n",
    "q=adjmatrix1[1].matmul(adj)\n",
    "l=adjmatrix1[0].matmul(q)\n",
    "start = time.time()\n",
    "\n",
    "print(\"svd s u v = \",adjmatrix1[0].matmul(adjmatrix1[1].matmul( adj))\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "start = time.time()\n",
    "print(\"svd s u v = \",l==adjmatrix1[0].matmul(adjmatrix1[1].matmul( adj)))\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "import pandas as pd\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def flip_signs(A, B):\n",
    "    \"\"\"\n",
    "    utility function for resolving the sign ambiguity in SVD\n",
    "    http://stats.stackexchange.com/q/34396/115202\n",
    "    \"\"\"\n",
    "    signs = np.sign(A) * np.sign(B)\n",
    "    return A, B * signs\n",
    "\n",
    "\n",
    "# Let the data matrix X be of n x p size,\n",
    "# where n is the number of samples and p is the number of variables\n",
    "n, p = 170, 170\n",
    "adj_df = pd.read_csv(\"data/pems08_adj.csv\", header=None)\n",
    "X = np.array(adj_df, dtype=np.float32)\n",
    "X -= np.mean(X,axis=0)\n",
    "\n",
    "# the p x p covariance matrix\n",
    "C = np.cov(X, rowvar=False)\n",
    "print(\"C = \\n\", C)\n",
    "# C is a symmetric matrix and so it can be diagonalized:\n",
    "l, principal_axes = la.eig(C)\n",
    "# sort results wrt. eigenvalues\n",
    "idx = l.argsort()[::-1]\n",
    "l, principal_axes = l[idx], principal_axes[:, idx]\n",
    "# the eigenvalues in decreasing order\n",
    "print(\"l = \\n\", l)\n",
    "# a matrix of eigenvectors (each column is an eigenvector)\n",
    "print(\"V = \\n\", principal_axes)\n",
    "# projections of X on the principal axes are called principal components\n",
    "principal_components = X.dot(principal_axes)\n",
    "print(\"Y = \\n\", principal_components)\n",
    "\n",
    "# we now perform singular value decomposition of X\n",
    "# \"economy size\" (or \"thin\") SVD\n",
    "U, s, Vt = la.svd(X, full_matrices=False)\n",
    "V = Vt.T\n",
    "S = np.diag(s)\n",
    "\n",
    "# 1) then columns of V are principal directions/axes.\n",
    "#assert np.allclose(V, principal_axes)\n",
    "\n",
    "# 2) columns of US are principal components\n",
    "assert np.allclose(*flip_signs(U.dot(S), principal_components))\n",
    "\n",
    "# 3) singular values are related to the eigenvalues of covariance matrix\n",
    "assert np.allclose((s ** 2) / (n - 1), l)\n",
    "\n",
    "# 8) dimensionality reduction\n",
    "k = 2\n",
    "PC_k = principal_components[:, 0:k]\n",
    "US_k = U[:, 0:k].dot(S[0:k, 0:k])\n",
    "assert np.allclose(*flip_signs(PC_k, US_k))\n",
    "\n",
    "# 10) we used \"economy size\" (or \"thin\") SVD\n",
    "assert U.shape == (n, p)\n",
    "assert S.shape == (p, p)\n",
    "assert V.shape == (p, p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
