{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.18.5)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (3.3.3)\n",
      "Collecting pytorch-lightning>=1.3.0\n",
      "  Downloading pytorch_lightning-1.5.10-py3-none-any.whl (527 kB)\n",
      "\u001b[K     |████████████████████████████████| 527 kB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning>=1.3.0->-r requirements.txt (line 5)) (3.7.4.3)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning>=1.3.0->-r requirements.txt (line 5)) (20.8)\n",
      "Collecting pyDeprecate==0.3.1\n",
      "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
      "Collecting torch\n",
      "  Downloading torch-1.10.2-cp36-cp36m-manylinux1_x86_64.whl (881.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 881.9 MB 23 kB/s  eta 0:00:011     |██████████████████████▌         | 620.3 MB 8.3 MB/s eta 0:00:32     |██████████████████████████      | 718.4 MB 13.4 MB/s eta 0:00:13\n",
      "\u001b[?25hCollecting torchmetrics>=0.3.0\n",
      "  Downloading torchmetrics-0.8.2-py3-none-any.whl (409 kB)\n",
      "\u001b[K     |████████████████████████████████| 409 kB 7.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
      "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 15.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting future>=0.17.1\n",
      "  Downloading future-0.18.3.tar.gz (840 kB)\n",
      "\u001b[K     |████████████████████████████████| 840 kB 7.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=17.0->pytorch-lightning>=1.3.0->-r requirements.txt (line 5)) (2.4.7)\n",
      "Collecting PyYAML>=5.1\n",
      "  Downloading PyYAML-6.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (603 kB)\n",
      "\u001b[K     |████████████████████████████████| 603 kB 10.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard>=2.2.0\n",
      "  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.9 MB 8.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.0->-r requirements.txt (line 5)) (1.34.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.0->-r requirements.txt (line 5)) (3.3.3)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.0->-r requirements.txt (line 5)) (0.30.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.0->-r requirements.txt (line 5)) (0.11.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.0->-r requirements.txt (line 5)) (3.14.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.0->-r requirements.txt (line 5)) (1.0.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning>=1.3.0->-r requirements.txt (line 5)) (1.11.0)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.17.3-py2.py3-none-any.whl (178 kB)\n",
      "\u001b[K     |████████████████████████████████| 178 kB 8.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning>=1.3.0->-r requirements.txt (line 5)) (3.3.0)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 12.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 4.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting requests\n",
      "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 3.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.3.0->-r requirements.txt (line 5)) (2.6)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 10.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[K     |████████████████████████████████| 151 kB 9.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 9.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 9.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm>=4.41.0\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 7.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[K     |████████████████████████████████| 140 kB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 2)) (2.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 2)) (8.1.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 2)) (0.10.0)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (9.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.5 MB 12.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz>=2017.2\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[K     |████████████████████████████████| 502 kB 10.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting python-dotenv\n",
      "  Downloading python_dotenv-0.20.0-py3-none-any.whl (17 kB)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.9 MB 15.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.8.4-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (946 kB)\n",
      "\u001b[K     |████████████████████████████████| 946 kB 7.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.3.0->-r requirements.txt (line 5)) (20.3.0)\n",
      "Collecting asynctest==0.13.0\n",
      "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.2.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (191 kB)\n",
      "\u001b[K     |████████████████████████████████| 191 kB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna-ssl>=1.0\n",
      "  Downloading idna-ssl-1.1.0.tar.gz (3.4 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-5.2.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (159 kB)\n",
      "\u001b[K     |████████████████████████████████| 159 kB 8.5 MB/s eta 0:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (270 kB)\n",
      "\u001b[K     |████████████████████████████████| 270 kB 7.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dataclasses\n",
      "  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning>=1.3.0->-r requirements.txt (line 5)) (3.4.0)\n",
      "Collecting importlib-resources\n",
      "  Downloading importlib_resources-5.4.0-py3-none-any.whl (28 kB)\n",
      "Collecting setuptools==59.5.0\n",
      "  Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\n",
      "\u001b[K     |████████████████████████████████| 952 kB 6.6 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: future, idna-ssl\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=494240 sha256=a38e5c39da6a0ef358ded9846806dbfd0bde690f1f96bf88b97c0faf54d4530a\n",
      "  Stored in directory: /root/.cache/pip/wheels/63/f1/0c/e56d12b3804345ce5ba34279cbfe583ecafdd1401551457330\n",
      "  Building wheel for idna-ssl (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-py3-none-any.whl size=3949 sha256=42236bd0ba073ec117bbfdcfcf7e8cba1ab3aea3573414e9f83daf00516a15d6\n",
      "  Stored in directory: /root/.cache/pip/wheels/6a/f5/9c/f8331a854f7a8739cf0e74c13854e4dd7b1af11b04fe1dde13\n",
      "Successfully built future idna-ssl\n",
      "Installing collected packages: urllib3, pyasn1, charset-normalizer, certifi, rsa, requests, pyasn1-modules, oauthlib, multidict, frozenlist, cachetools, yarl, requests-oauthlib, idna-ssl, google-auth, dataclasses, asynctest, async-timeout, aiosignal, torch, tensorboard-plugin-wit, tensorboard-data-server, setuptools, pyDeprecate, importlib-resources, google-auth-oauthlib, fsspec, aiohttp, tqdm, torchmetrics, tensorboard, PyYAML, pytz, future, scipy, pytorch-lightning, python-dotenv, pandas\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 51.1.1\n",
      "    Uninstalling setuptools-51.1.1:\n",
      "      Successfully uninstalled setuptools-51.1.1\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 1.15.0\n",
      "    Uninstalling tensorboard-1.15.0:\n",
      "      Successfully uninstalled tensorboard-1.15.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 1.15.5 requires tensorboard<1.16.0,>=1.15.0, but you have tensorboard 2.10.1 which is incompatible.\u001b[0m\n",
      "Successfully installed PyYAML-6.0 aiohttp-3.8.4 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 cachetools-4.2.4 certifi-2022.12.7 charset-normalizer-2.0.12 dataclasses-0.8 frozenlist-1.2.0 fsspec-2022.1.0 future-0.18.3 google-auth-2.17.3 google-auth-oauthlib-0.4.6 idna-ssl-1.1.0 importlib-resources-5.4.0 multidict-5.2.0 oauthlib-3.2.2 pandas-1.1.5 pyDeprecate-0.3.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 python-dotenv-0.20.0 pytorch-lightning-1.5.10 pytz-2023.3 requests-2.27.1 requests-oauthlib-1.3.1 rsa-4.9 scipy-1.5.4 setuptools-59.5.0 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 torch-1.10.2 torchmetrics-0.8.2 tqdm-4.64.1 urllib3-1.26.15 yarl-1.7.2\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## !python main.py --model_name TGCN \\\n",
    "--max_epochs 2 \\\n",
    "--learning_rate 0.001 \\\n",
    "--weight_decay 0 \\\n",
    "--batch_size 64 \\\n",
    "--hidden_dim 64 \\\n",
    "--settings supervised \\\n",
    "--loss \"mse_with_regularizer\" \\\n",
    "--data PEMS04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"GRU\", \"LSTM\", \"GCN\", \"TGCN\",\"TGCN_LSTM\",\"TGCN_UGRNN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: PEMS08  Loss: mse_with_regularizer  Model: GRU\n"
     ]
    }
   ],
   "source": [
    "from subprocess import Popen,PIPE\n",
    "model = [\"GRU\"]\n",
    "max_epochs = \"100\"\n",
    "learning_rate = \"0.001\" \n",
    "weight_decay =\"0\"\n",
    "enable_progress_bar = \"False\"\n",
    "batch_size = \"64\"\n",
    "hidden_dim = \"64\"\n",
    "cell_dim = \"64\"\n",
    "dropout = \"0\"\n",
    "layer_2 = [\"False\"]\n",
    "loss = [\"mse_with_regularizer\",\"mse_with_regularizer_l1\",\"mse_with_regularizer_entropy\",\"mse\"]\n",
    "data = [\"PEMS08\",\"PEMS04\",\"losloop\",\"shenzhen\"]\n",
    "gpus = \"0\"\n",
    "for i in data:\n",
    "    for j in loss:\n",
    "        for k in model:\n",
    "            for l in layer_2:\n",
    "                print(\"Dataset:\",i,\" Loss:\",j,\" Model:\",k,\"Layer 2:\",l)\n",
    "                cmd_str = [\"python\",\n",
    "                       \"main.py\",\n",
    "                       \"--enable_progress_bar\",enable_progress_bar,\n",
    "                       \"--model_name\",k,\n",
    "                       \"--max_epochs\",max_epochs,\n",
    "                       \"--learning_rate\",learning_rate,\n",
    "                       \"--weight_decay\",weight_decay,\n",
    "                       \"--batch_size\",batch_size,\n",
    "                       \"--hidden_dim\",hidden_dim,\n",
    "                       \"--cell_dim\",hidden_dim,\n",
    "                       \"--dropout\",dropout,\n",
    "                       \"--loss\",j,\n",
    "                       \"--layer_2\",layer_2[l],\n",
    "                       \"--data\",i,\n",
    "                       \"--gpus\",gpus,\n",
    "                      ]\n",
    "                p = Popen(cmd_str,stdout=PIPE)\n",
    "                for m in p.stdout:\n",
    "                    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[2023-04-14 09:29:59,066 INFO]\u001b[0m{'logger': True, 'checkpoint_callback': None, 'enable_checkpointing': True, 'default_root_dir': None, 'gradient_clip_val': None, 'gradient_clip_algorithm': None, 'process_position': 0, 'num_nodes': 1, 'num_processes': 1, 'devices': None, 'gpus': None, 'auto_select_gpus': False, 'tpu_cores': None, 'ipus': None, 'log_gpu_memory': None, 'progress_bar_refresh_rate': None, 'enable_progress_bar': True, 'overfit_batches': 0.0, 'track_grad_norm': -1, 'check_val_every_n_epoch': 1, 'fast_dev_run': False, 'accumulate_grad_batches': None, 'max_epochs': 5, 'min_epochs': None, 'max_steps': -1, 'min_steps': None, 'max_time': None, 'limit_train_batches': 1.0, 'limit_val_batches': 1.0, 'limit_test_batches': 1.0, 'limit_predict_batches': 1.0, 'val_check_interval': 1.0, 'flush_logs_every_n_steps': None, 'log_every_n_steps': 50, 'accelerator': None, 'strategy': None, 'sync_batchnorm': False, 'precision': 32, 'enable_model_summary': True, 'weights_summary': 'top', 'weights_save_path': None, 'num_sanity_val_steps': 2, 'resume_from_checkpoint': None, 'profiler': None, 'benchmark': False, 'deterministic': False, 'reload_dataloaders_every_n_epochs': 0, 'reload_dataloaders_every_epoch': False, 'auto_lr_find': False, 'replace_sampler_ddp': True, 'detect_anomaly': False, 'auto_scale_batch_size': False, 'prepare_data_per_node': None, 'plugins': None, 'amp_backend': 'native', 'amp_level': None, 'move_metrics_to_cpu': False, 'multiple_trainloader_mode': 'max_size_cycle', 'stochastic_weight_avg': False, 'terminate_on_nan': None, 'data': 'PEMS08', 'model_name': 'TGCN_LSTM', 'settings': 'supervised', 'log_path': None, 'send_email': False, 'batch_size': 32, 'seq_len': 12, 'pre_len': 3, 'split_ratio': 0.8, 'normalize': True, 'hidden_dim': 64, 'dropout': 0, 'cell_dim': 64, 'layer_2': False, 'learning_rate': 0.001, 'weight_decay': 0.0, 'loss': 'mse_with_regularizer'}\n",
      "\u001b[31m[2023-04-14 09:29:59,775 INFO]\u001b[0mGPU available: False, used: False\n",
      "\u001b[31m[2023-04-14 09:29:59,775 INFO]\u001b[0mTPU available: False, using: 0 TPU cores\n",
      "\u001b[31m[2023-04-14 09:29:59,776 INFO]\u001b[0mIPU available: False, using: 0 IPUs\n",
      "\u001b[31m[2023-04-14 09:30:02,776 INFO]\u001b[0m\n",
      "  | Name      | Type      | Params\n",
      "----------------------------------------\n",
      "0 | model     | TGCN_LSTM | 16.9 K\n",
      "1 | regressor | Linear    | 195   \n",
      "----------------------------------------\n",
      "17.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "17.1 K    Total params\n",
      "0.068     Total estimated model params size (MB)\n",
      "Epoch 0:  92%|█████████▏| 410/447 [02:02<00:11,  3.35it/s, loss=0.146, v_num=38]^C\n",
      "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py:1399: UserWarning: `.validate(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `validate(ckpt_path='best')` to use and best model checkpoint and avoid this warning or `ckpt_path=trainer.checkpoint_callback.last_model_path` to use the last model.\n",
      "  f\"`.{fn}(ckpt_path=None)` was called without a model.\"\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 112, in <module>\n",
      "    results = main(args)\n",
      "  File \"main.py\", line 72, in main\n",
      "    results = globals()[\"main_\" + args.settings](args)\n",
      "  File \"main.py\", line 66, in main_supervised\n",
      "    results = trainer.validate(datamodule=dm)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 821, in validate\n",
      "    return self._call_and_handle_interrupt(self._validate_impl, model, dataloaders, ckpt_path, verbose, datamodule)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 685, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 860, in _validate_impl\n",
      "    ckpt_path, model_provided=model_provided, model_connected=self.lightning_module is not None\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1420, in __set_ckpt_path\n",
      "    f'`.{fn}(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model.'\n",
      "pytorch_lightning.utilities.exceptions.MisconfigurationException: `.validate(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model.\n",
      "CPU times: user 1.83 s, sys: 456 ms, total: 2.29 s\n",
      "Wall time: 2min 23s\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[2023-04-14 10:30:04,505 INFO]\u001b[0m{'logger': True, 'checkpoint_callback': None, 'enable_checkpointing': True, 'default_root_dir': None, 'gradient_clip_val': None, 'gradient_clip_algorithm': None, 'process_position': 0, 'num_nodes': 1, 'num_processes': 1, 'devices': None, 'gpus': None, 'auto_select_gpus': False, 'tpu_cores': None, 'ipus': None, 'log_gpu_memory': None, 'progress_bar_refresh_rate': None, 'enable_progress_bar': True, 'overfit_batches': 0.0, 'track_grad_norm': -1, 'check_val_every_n_epoch': 1, 'fast_dev_run': False, 'accumulate_grad_batches': None, 'max_epochs': 5, 'min_epochs': None, 'max_steps': -1, 'min_steps': None, 'max_time': None, 'limit_train_batches': 1.0, 'limit_val_batches': 1.0, 'limit_test_batches': 1.0, 'limit_predict_batches': 1.0, 'val_check_interval': 1.0, 'flush_logs_every_n_steps': None, 'log_every_n_steps': 50, 'accelerator': None, 'strategy': None, 'sync_batchnorm': False, 'precision': 32, 'enable_model_summary': True, 'weights_summary': 'top', 'weights_save_path': None, 'num_sanity_val_steps': 2, 'resume_from_checkpoint': None, 'profiler': None, 'benchmark': False, 'deterministic': False, 'reload_dataloaders_every_n_epochs': 0, 'reload_dataloaders_every_epoch': False, 'auto_lr_find': False, 'replace_sampler_ddp': True, 'detect_anomaly': False, 'auto_scale_batch_size': False, 'prepare_data_per_node': None, 'plugins': None, 'amp_backend': 'native', 'amp_level': None, 'move_metrics_to_cpu': False, 'multiple_trainloader_mode': 'max_size_cycle', 'stochastic_weight_avg': False, 'terminate_on_nan': None, 'data': 'losloop', 'model_name': 'TGCN_LSTM', 'settings': 'supervised', 'log_path': None, 'send_email': False, 'batch_size': 32, 'seq_len': 12, 'pre_len': 12, 'split_ratio': 0.8, 'normalize': True, 'hidden_dim': 64, 'dropout': 0, 'cell_dim': 64, 'layer_2': True, 'learning_rate': 0.001, 'weight_decay': 0.0, 'loss': 'mse_with_regularizer'}\n",
      "\u001b[31m[2023-04-14 10:30:04,652 INFO]\u001b[0mGPU available: False, used: False\n",
      "\u001b[31m[2023-04-14 10:30:04,652 INFO]\u001b[0mTPU available: False, using: 0 TPU cores\n",
      "\u001b[31m[2023-04-14 10:30:04,652 INFO]\u001b[0mIPU available: False, using: 0 IPUs\n",
      "\u001b[31m[2023-04-14 10:30:09,517 INFO]\u001b[0m\n",
      "  | Name      | Type      | Params\n",
      "----------------------------------------\n",
      "0 | model     | TGCN_LSTM | 33.8 K\n",
      "1 | regressor | Linear    | 780   \n",
      "----------------------------------------\n",
      "34.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "34.6 K    Total params\n",
      "0.138     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]h torch.Size([380, 13248])\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 112, in <module>\n",
      "    results = main(args)\n",
      "  File \"main.py\", line 72, in main\n",
      "    results = globals()[\"main_\" + args.settings](args)\n",
      "  File \"main.py\", line 65, in main_supervised\n",
      "    trainer.fit(task, dm)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 741, in fit\n",
      "    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 685, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 777, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1199, in _run\n",
      "    self._dispatch()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1279, in _dispatch\n",
      "    self.training_type_plugin.start_training(self)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 202, in start_training\n",
      "    self._results = trainer.run_stage()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1289, in run_stage\n",
      "    return self._run_train()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1311, in _run_train\n",
      "    self._run_sanity_check(self.lightning_module)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1375, in _run_sanity_check\n",
      "    self._evaluation_loop.run()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/loops/base.py\", line 145, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 110, in advance\n",
      "    dl_outputs = self.epoch_loop.run(dataloader, dataloader_idx, dl_max_batches, self.num_dataloaders)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/loops/base.py\", line 145, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 122, in advance\n",
      "    output = self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 217, in _evaluation_step\n",
      "    output = self.trainer.accelerator.validation_step(step_kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/accelerators/accelerator.py\", line 239, in validation_step\n",
      "    return self.training_type_plugin.validation_step(*step_kwargs.values())\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 219, in validation_step\n",
      "    return self.model.validation_step(*args, **kwargs)\n",
      "  File \"/tf/notebooks/T-GCN/T-GCN/T-GCN-PyTorch/tasks/supervised.py\", line 82, in validation_step\n",
      "    predictions, y = self.shared_step(batch, batch_idx)\n",
      "  File \"/tf/notebooks/T-GCN/T-GCN/T-GCN-PyTorch/tasks/supervised.py\", line 57, in shared_step\n",
      "    predictions = self(x)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/tf/notebooks/T-GCN/T-GCN/T-GCN-PyTorch/tasks/supervised.py\", line 42, in forward\n",
      "    hidden = self.model(x)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/tf/notebooks/T-GCN/T-GCN/T-GCN-PyTorch/models/tgcnlstm.py\", line 146, in forward\n",
      "    output, hidden_state_2 , cell_state_2 = self.tgcn_cell_2(output, hidden_state_2, cell_state_2)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/tf/notebooks/T-GCN/T-GCN/T-GCN-PyTorch/models/tgcnlstm.py\", line 88, in forward\n",
      "    concatenation = torch.sigmoid(self.graph_conv1(inputs, hidden_state))\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/tf/notebooks/T-GCN/T-GCN/T-GCN-PyTorch/models/tgcnlstm.py\", line 32, in forward\n",
      "    (batch_size, num_nodes, self._num_lstm_units)\n",
      "RuntimeError: shape '[380, 13248, 64]' is invalid for input of size 5034240\n",
      "CPU times: user 149 ms, sys: 56 ms, total: 205 ms\n",
      "Wall time: 8.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!python main.py \\\n",
    "--model_name TGCN_LSTM \\\n",
    "--data losloop \\\n",
    "--max_epochs 5 \\\n",
    "--learning_rate 0.001 \\\n",
    "--weight_decay 0 \\\n",
    "--batch_size 32 \\\n",
    "--hidden_dim 64 \\\n",
    "--settings supervised \\\n",
    "--loss \"mse_with_regularizer\" \\\n",
    "--layer_2 True \\\n",
    "--pre_len 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATALOADER:0 VALIDATE RESULTS\n",
    "{'ExplainedVar': 0.5314704179763794,\n",
    " 'MAE': 6.8871846199035645,\n",
    " 'R2': 0.5294057130813599,\n",
    " 'RMSE': 9.533448219299316,\n",
    " 'accuracy': 0.8377037048339844,\n",
    " 'val_loss': 91.16824340820312}\n",
    "--------------------------------------------------------------------------------\n",
    "Validating: 100%|█████████████████████████████████| 1/1 [00:02<00:00,  2.11s/it]\n",
    "CPU times: user 2.26 s, sys: 564 ms, total: 2.82 s\n",
    "Wall time: 2min 47s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[2023-04-14 09:17:39,594 INFO]\u001b[0m{'logger': True, 'checkpoint_callback': None, 'enable_checkpointing': True, 'default_root_dir': None, 'gradient_clip_val': None, 'gradient_clip_algorithm': None, 'process_position': 0, 'num_nodes': 1, 'num_processes': 1, 'devices': None, 'gpus': None, 'auto_select_gpus': False, 'tpu_cores': None, 'ipus': None, 'log_gpu_memory': None, 'progress_bar_refresh_rate': None, 'enable_progress_bar': True, 'overfit_batches': 0.0, 'track_grad_norm': -1, 'check_val_every_n_epoch': 1, 'fast_dev_run': False, 'accumulate_grad_batches': None, 'max_epochs': 5, 'min_epochs': None, 'max_steps': -1, 'min_steps': None, 'max_time': None, 'limit_train_batches': 1.0, 'limit_val_batches': 1.0, 'limit_test_batches': 1.0, 'limit_predict_batches': 1.0, 'val_check_interval': 1.0, 'flush_logs_every_n_steps': None, 'log_every_n_steps': 50, 'accelerator': None, 'strategy': None, 'sync_batchnorm': False, 'precision': 32, 'enable_model_summary': True, 'weights_summary': 'top', 'weights_save_path': None, 'num_sanity_val_steps': 2, 'resume_from_checkpoint': None, 'profiler': None, 'benchmark': False, 'deterministic': False, 'reload_dataloaders_every_n_epochs': 0, 'reload_dataloaders_every_epoch': False, 'auto_lr_find': False, 'replace_sampler_ddp': True, 'detect_anomaly': False, 'auto_scale_batch_size': False, 'prepare_data_per_node': None, 'plugins': None, 'amp_backend': 'native', 'amp_level': None, 'move_metrics_to_cpu': False, 'multiple_trainloader_mode': 'max_size_cycle', 'stochastic_weight_avg': False, 'terminate_on_nan': None, 'data': 'shenzhen', 'model_name': 'TGCN', 'settings': 'supervised', 'log_path': None, 'send_email': False, 'batch_size': 32, 'seq_len': 4, 'pre_len': 4, 'split_ratio': 0.8, 'normalize': True, 'hidden_dim': 64, 'dropout': 0.0, 'cell_dim': 64, 'layer_2': True, 'learning_rate': 0.001, 'weight_decay': 0.0, 'loss': 'mse'}\n",
      "\u001b[31m[2023-04-14 09:17:39,733 INFO]\u001b[0mGPU available: False, used: False\n",
      "\u001b[31m[2023-04-14 09:17:39,733 INFO]\u001b[0mTPU available: False, using: 0 TPU cores\n",
      "\u001b[31m[2023-04-14 09:17:39,733 INFO]\u001b[0mIPU available: False, using: 0 IPUs\n",
      "\u001b[31m[2023-04-14 09:17:43,197 INFO]\u001b[0m\n",
      "  | Name      | Type   | Params\n",
      "-------------------------------------\n",
      "0 | model     | TGCN   | 12.7 K\n",
      "1 | regressor | Linear | 260   \n",
      "-------------------------------------\n",
      "12.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "12.9 K    Total params\n",
      "0.052     Total estimated model params size (MB)\n",
      "Epoch 0:  99%|█████████▊| 75/76 [00:13<00:00,  5.53it/s, loss=0.00645, v_num=35]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 76/76 [00:14<00:00,  5.07it/s, loss=0.00645, v_num=35]\u001b[A\n",
      "Epoch 1:  99%|█████████▊| 75/76 [00:11<00:00,  6.61it/s, loss=0.00489, v_num=35]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 76/76 [00:12<00:00,  6.02it/s, loss=0.00489, v_num=35]\u001b[A\n",
      "Epoch 2:  99%|█████████▊| 75/76 [00:10<00:00,  6.87it/s, loss=0.00479, v_num=35]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 76/76 [00:12<00:00,  6.28it/s, loss=0.00479, v_num=35]\u001b[A\n",
      "Epoch 3:  99%|█████████▊| 75/76 [00:11<00:00,  6.59it/s, loss=0.00471, v_num=35]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 76/76 [00:12<00:00,  6.00it/s, loss=0.00471, v_num=35]\u001b[A\n",
      "Epoch 4:  99%|█████████▊| 75/76 [00:11<00:00,  6.55it/s, loss=0.00462, v_num=35]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 76/76 [00:12<00:00,  5.94it/s, loss=0.00462, v_num=35]\u001b[A\n",
      "Epoch 4: 100%|██████████| 76/76 [00:12<00:00,  5.94it/s, loss=0.00462, v_num=35]\u001b[A\n",
      "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py:1399: UserWarning: `.validate(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `validate(ckpt_path='best')` to use and best model checkpoint and avoid this warning or `ckpt_path=trainer.checkpoint_callback.last_model_path` to use the last model.\n",
      "  f\"`.{fn}(ckpt_path=None)` was called without a model.\"\n",
      "\u001b[31m[2023-04-14 09:19:34,041 INFO]\u001b[0mRestoring states from the checkpoint path at /tf/notebooks/T-GCN/T-GCN/T-GCN-PyTorch/lightning_logs/version_35/checkpoints/epoch=4-step=374.ckpt\n",
      "\u001b[31m[2023-04-14 09:19:34,046 INFO]\u001b[0mLoaded model weights from checkpoint at /tf/notebooks/T-GCN/T-GCN/T-GCN-PyTorch/lightning_logs/version_35/checkpoints/epoch=4-step=374.ckpt\n",
      "Validating: 100%|█████████████████████████████████| 1/1 [00:01<00:00,  1.21s/it]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 VALIDATE RESULTS\n",
      "{'ExplainedVar': 0.6692681908607483,\n",
      " 'MAE': 4.516780853271484,\n",
      " 'R2': 0.6692502498626709,\n",
      " 'RMSE': 6.00551700592041,\n",
      " 'accuracy': 0.5816536545753479,\n",
      " 'val_loss': 36.06623077392578}\n",
      "--------------------------------------------------------------------------------\n",
      "Validating: 100%|█████████████████████████████████| 1/1 [00:01<00:00,  1.22s/it]\n",
      "CPU times: user 1.67 s, sys: 451 ms, total: 2.12 s\n",
      "Wall time: 1min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python main.py --model_name TGCN \\\n",
    "--max_epochs 5 \\\n",
    "--learning_rate 0.001 \\\n",
    "--weight_decay 0 \\\n",
    "--batch_size 32 \\\n",
    "--hidden_dim 64 \\\n",
    "--settings supervised \\\n",
    "--loss \"mse\" \\\n",
    "--layer_2 False \\\n",
    "--dropout 0 \\\n",
    "--seq_len 4 \\\n",
    "--pre_len 4 \\\n",
    "--data shenzhen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --model_name GRU \\\n",
    "--max_epochs 10 \\\n",
    "--learning_rate 0.001 \\\n",
    "--weight_decay 0 \\\n",
    "--batch_size 64 \\\n",
    "--hidden_dim 8 \\\n",
    "--settings supervised \\\n",
    "--data PEMS04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --model_name LSTM \\\n",
    "--max_epochs 1 \\\n",
    "--learning_rate 0.001 \\\n",
    "--weight_decay 0 \\\n",
    "--batch_size 64 \\\n",
    "--hidden_dim 8 \\\n",
    "--cell_dim 12 \\\n",
    "--settings supervised \\\n",
    "--data PEMS04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TGCN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --model_name TGCN_LSTM \\\n",
    "--max_epochs 20 \\\n",
    "--learning_rate 0.001 \\\n",
    "--weight_decay 0 \\\n",
    "--batch_size 64 \\\n",
    "--hidden_dim 8 \\\n",
    "--cell_dim 64 \\\n",
    "--settings supervised \\\n",
    "--data losloop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svd s u v =  tensor([], size=(0, 170))\n",
      "0.0009138584136962891\n",
      "svd s u v =  tensor([[True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        ...,\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True]])\n",
      "0.007382869720458984\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "import time\n",
    "\n",
    "def svd_low_rank_approximation(matrix,k):\n",
    "    # Perform SVD on the matrix\n",
    "    u, s, v = torch.linalg.svd(matrix, full_matrices=False)\n",
    "    \n",
    "    k=0\n",
    "    # Truncate the singular values and matrices to rank k\n",
    "    s_k = s[:k]\n",
    "    u_k = u[:, :k]\n",
    "    v_k = v[:, :k]\n",
    "    return u_k.mm(torch.diag(s_k)),v_k.t()\n",
    "\n",
    "def adjmat(adj):\n",
    "    return torch.mm(torch.mm(adj[1], torch.diag(adj[0])), adj[2].t())\n",
    "\n",
    "PEMS08_adj = pd.read_csv(r'data/pems08_adj.csv',header=None)\n",
    "adj = np.array(PEMS08_adj,dtype=np.float32)\n",
    "adj = torch.tensor(adj)\n",
    "\n",
    "data = np.load(\"data/pems08.npz\")\n",
    "data = data.f.data\n",
    "df_data = pd.DataFrame(data[:,:,0])\n",
    "df_data = df_data.values.tolist()\n",
    "feat = np.array(df_data, dtype=np.float32)\n",
    "feat = torch.tensor(feat)\n",
    "\n",
    "for i in range(1):\n",
    "    adjsvd= svd_low_rank_approximation(feat,115)\n",
    "    adjmatrix1= adjsvd\n",
    "for i in range(1):\n",
    "    adjsvd= svd_low_rank_approximation(feat,170)\n",
    "    adjmatrix= adjsvd\n",
    "q=adjmatrix1[1].matmul( adj)\n",
    "l=adjmatrix1[0].matmul(q)\n",
    "start = time.time()\n",
    "\n",
    "print(\"svd s u v = \",adjmatrix1[0].matmul(adjmatrix1[1].matmul( adj))\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "start = time.time()\n",
    "print(\"svd s u v = \",l==adjmatrix1[0].matmul(adjmatrix1[1].matmul( adj)))\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = \n",
      " [[ 5.88235260e-03  0.00000000e+00 -3.48068201e-05 ... -1.04420460e-04\n",
      "  -3.48068201e-05 -1.39227285e-04]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [-3.48068201e-05  0.00000000e+00  5.88235260e-03 ... -1.04420460e-04\n",
      "  -3.48068201e-05 -1.39227285e-04]\n",
      " ...\n",
      " [-1.04420460e-04  0.00000000e+00 -1.04420460e-04 ...  1.74382169e-02\n",
      "  -1.04420460e-04 -4.17681855e-04]\n",
      " [-3.48068201e-05  0.00000000e+00 -3.48068201e-05 ... -1.04420460e-04\n",
      "   5.88235260e-03 -1.39227285e-04]\n",
      " [-1.39227285e-04  0.00000000e+00 -1.39227285e-04 ... -4.17681855e-04\n",
      "  -1.39227285e-04  2.31117300e-02]]\n",
      "l = \n",
      " [ 8.24535603e-02  5.55144401e-02  4.90399642e-02  4.18993695e-02\n",
      "  3.89620037e-02  3.58178200e-02  3.43364616e-02  3.26124024e-02\n",
      "  3.17529181e-02  3.15183464e-02  2.87402235e-02  2.72061396e-02\n",
      "  2.59586780e-02  2.51821272e-02  2.31557993e-02  2.27529938e-02\n",
      "  2.22720107e-02  2.05657302e-02  2.03462432e-02  1.88002046e-02\n",
      "  1.87496963e-02  1.83902272e-02  1.79994776e-02  1.79357182e-02\n",
      "  1.77514792e-02  1.77514790e-02  1.77514790e-02  1.76163181e-02\n",
      "  1.73084925e-02  1.69625508e-02  1.60557718e-02  1.57780983e-02\n",
      "  1.48575928e-02  1.38528040e-02  1.36394763e-02  1.30665952e-02\n",
      "  1.26417784e-02  1.22763428e-02  1.16377346e-02  1.14562667e-02\n",
      "  1.06400266e-02  1.02857955e-02  1.01511535e-02  1.00570977e-02\n",
      "  9.71913638e-03  8.66383318e-03  8.44607755e-03  8.32804348e-03\n",
      "  8.15089278e-03  8.07922145e-03  7.64043669e-03  6.55580926e-03\n",
      "  6.07008718e-03  5.91715977e-03  5.91715977e-03  5.91715966e-03\n",
      "  5.91715950e-03  5.91715943e-03  5.91715942e-03  5.91715942e-03\n",
      "  5.91715942e-03  5.91715942e-03  5.91715942e-03  5.91715942e-03\n",
      "  5.91715942e-03  5.91715942e-03  5.91715942e-03  5.91715942e-03\n",
      "  5.91715942e-03  5.91715942e-03  5.91715942e-03  5.91715942e-03\n",
      "  5.91715942e-03  5.91715942e-03  5.91715942e-03  5.91715942e-03\n",
      "  5.91715942e-03  5.91715942e-03  5.91715942e-03  5.91715942e-03\n",
      "  5.91715942e-03  5.91715942e-03  5.91715942e-03  5.91715942e-03\n",
      "  5.91715942e-03  5.91715942e-03  5.91715942e-03  5.91715942e-03\n",
      "  5.91715942e-03  5.91715942e-03  5.91715942e-03  5.91715942e-03\n",
      "  5.91715942e-03  5.91715942e-03  5.91715942e-03  5.91715942e-03\n",
      "  5.91715942e-03  5.91715942e-03  5.91715942e-03  5.91715942e-03\n",
      "  5.91715942e-03  5.91715942e-03  5.91715942e-03  5.91715942e-03\n",
      "  5.91715942e-03  5.91715942e-03  5.91715942e-03  5.91715942e-03\n",
      "  5.91715942e-03  5.91715942e-03  5.91715942e-03  5.91715942e-03\n",
      "  5.91715942e-03  5.91715942e-03  5.91715942e-03  5.91715942e-03\n",
      "  5.91715942e-03  5.91715942e-03  5.91715942e-03  5.91715942e-03\n",
      "  5.91715942e-03  5.91715942e-03  5.91715942e-03  5.91715942e-03\n",
      "  5.91715942e-03  5.91715942e-03  5.91715942e-03  5.91715942e-03\n",
      "  5.91715942e-03  5.91715942e-03  5.91715942e-03  5.91715942e-03\n",
      "  4.97240617e-03  4.88511954e-03  4.79475311e-03  3.96434595e-03\n",
      "  3.91653373e-03  3.57190892e-03  3.13797948e-03  2.81578594e-03\n",
      "  2.73931206e-03  2.72481609e-03  2.57039773e-03  2.51502318e-03\n",
      "  2.26365529e-03  1.79608821e-03  1.66301090e-03  1.36166012e-03\n",
      "  1.03702866e-03  9.87961865e-04  6.71183228e-04  5.36090139e-04\n",
      "  3.87789872e-04  3.61398877e-04  2.19581899e-04  1.75397468e-04\n",
      "  7.92003525e-05  3.03565710e-05  2.28278930e-05  1.50962734e-17\n",
      "  2.96724021e-18  2.07669139e-18  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -7.57436704e-18]\n",
      "V = \n",
      " [[-4.04981405e-03  4.61423777e-03 -4.76249093e-03 ...  0.00000000e+00\n",
      "   0.00000000e+00  5.79508280e-15]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [-4.04981405e-03  4.61423777e-03 -4.76249093e-03 ...  0.00000000e+00\n",
      "   0.00000000e+00  6.11984281e-15]\n",
      " ...\n",
      " [-2.13904314e-02  4.41062277e-02 -7.10296894e-02 ...  0.00000000e+00\n",
      "   0.00000000e+00  7.27282351e-15]\n",
      " [-4.04981405e-03  4.61423777e-03 -4.76249093e-03 ...  0.00000000e+00\n",
      "   0.00000000e+00  5.82168000e-15]\n",
      " [ 3.30735968e-01 -7.08219756e-02 -1.35094265e-01 ...  0.00000000e+00\n",
      "   0.00000000e+00  1.78787186e-14]]\n",
      "Y = \n",
      " [[-5.23829371e-02  3.86762692e-02 -3.47078654e-02 ...  0.00000000e+00\n",
      "   0.00000000e+00  2.05934510e-10]\n",
      " [-5.64327511e-02  4.32905069e-02 -3.94703562e-02 ...  0.00000000e+00\n",
      "   0.00000000e+00  2.05940630e-10]\n",
      " [-8.88542967e-02 -2.04120769e-01  3.14726730e-02 ...  0.00000000e+00\n",
      "   0.00000000e+00  2.05945698e-10]\n",
      " ...\n",
      " [-1.05687690e-01  1.52701794e-01 -2.21033096e-01 ...  0.00000000e+00\n",
      "   0.00000000e+00  2.05940204e-10]\n",
      " [-1.11452915e-01  1.62952918e-01 -2.36419819e-01 ...  0.00000000e+00\n",
      "   0.00000000e+00  2.05939787e-10]\n",
      " [ 1.18503480e+00 -1.09717065e-01 -3.86465594e-03 ...  0.00000000e+00\n",
      "   0.00000000e+00  2.05950763e-10]]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-ed6127263d5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# 2) columns of US are principal components\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mflip_signs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprincipal_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# 3) singular values are related to the eigenvalues of covariance matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "import pandas as pd\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def flip_signs(A, B):\n",
    "    \"\"\"\n",
    "    utility function for resolving the sign ambiguity in SVD\n",
    "    http://stats.stackexchange.com/q/34396/115202\n",
    "    \"\"\"\n",
    "    signs = np.sign(A) * np.sign(B)\n",
    "    return A, B * signs\n",
    "\n",
    "\n",
    "# Let the data matrix X be of n x p size,\n",
    "# where n is the number of samples and p is the number of variables\n",
    "n, p = 170, 170\n",
    "adj_df = pd.read_csv(\"data/pems08_adj.csv\", header=None)\n",
    "X = np.array(adj_df, dtype=np.float32)\n",
    "X -= np.mean(X,axis=0)\n",
    "\n",
    "# the p x p covariance matrix\n",
    "C = np.cov(X, rowvar=False)\n",
    "print(\"C = \\n\", C)\n",
    "# C is a symmetric matrix and so it can be diagonalized:\n",
    "l, principal_axes = la.eig(C)\n",
    "# sort results wrt. eigenvalues\n",
    "idx = l.argsort()[::-1]\n",
    "l, principal_axes = l[idx], principal_axes[:, idx]\n",
    "# the eigenvalues in decreasing order\n",
    "print(\"l = \\n\", l)\n",
    "# a matrix of eigenvectors (each column is an eigenvector)\n",
    "print(\"V = \\n\", principal_axes)\n",
    "# projections of X on the principal axes are called principal components\n",
    "principal_components = X.dot(principal_axes)\n",
    "print(\"Y = \\n\", principal_components)\n",
    "\n",
    "# we now perform singular value decomposition of X\n",
    "# \"economy size\" (or \"thin\") SVD\n",
    "U, s, Vt = la.svd(X, full_matrices=False)\n",
    "V = Vt.T\n",
    "S = np.diag(s)\n",
    "\n",
    "# 1) then columns of V are principal directions/axes.\n",
    "#assert np.allclose(V, principal_axes)\n",
    "\n",
    "# 2) columns of US are principal components\n",
    "assert np.allclose(*flip_signs(U.dot(S), principal_components))\n",
    "\n",
    "# 3) singular values are related to the eigenvalues of covariance matrix\n",
    "assert np.allclose((s ** 2) / (n - 1), l)\n",
    "\n",
    "# 8) dimensionality reduction\n",
    "k = 2\n",
    "PC_k = principal_components[:, 0:k]\n",
    "US_k = U[:, 0:k].dot(S[0:k, 0:k])\n",
    "assert np.allclose(*flip_signs(PC_k, US_k))\n",
    "\n",
    "# 10) we used \"economy size\" (or \"thin\") SVD\n",
    "assert U.shape == (n, p)\n",
    "assert S.shape == (p, p)\n",
    "assert V.shape == (p, p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
